{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from loss_function import loss_function\n",
    "from loss_function import cs\n",
    "from torch.autograd import Variable\n",
    "from evaluate import Evaluation\n",
    "from lstm import LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_reader as dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"../data/text_tokenized.txt.gz\"\n",
    "corpus = dr.read_corpus(corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_path = \"../data/vectors_pruned.200.txt.gz\"\n",
    "#embedding_path = \"../data/glove.6B.200d.txt.gz\"\n",
    "embedding_path = \"../data/glove.combined.300d.txt.gz\"\n",
    "embedding_tensor, word_to_indx = dr.getEmbeddingTensor(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_corpus = dr.map_corpus(corpus, word_to_indx, kernel_width = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../data/train_random.txt\"\n",
    "train = dr.read_annotations(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ex = dr.create_train_set(ids_corpus, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, dim):\n",
    "    l2 = torch.norm(x, 2, dim)#.expand_as(x)\n",
    "    l2 = torch.unsqueeze(l2, 2)\n",
    "    l2 = l2.expand_as(x)\n",
    "    return x / l2.clamp(min = 1e-8)\n",
    "\n",
    "def mean_pool_pad(x, mask):\n",
    "    mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "def train_model(train_data, dev_data, model):\n",
    "    model.cuda()\n",
    "    #model.cpu()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0004, weight_decay = 1e-5)\n",
    "    model.train()\n",
    "\n",
    "    lasttime = time.time()\n",
    "    for epoch in range(1, 31):\n",
    "        print(\"-------------\\nEpoch {}:\\n\".format(epoch))\n",
    "\n",
    "        loss = run_epoch(train_data, True, model, optimizer)\n",
    "        #return loss\n",
    "        print('Train loss: {:.6f}'.format(loss))\n",
    "        torch.save(model, \"model_epoch{}\".format(0 + epoch))\n",
    "        \n",
    "        (MAP, MRR, P1, P5) = run_epoch(dev_data, False, model, optimizer)\n",
    "        print('Val MAP: {:.6f}, MRR: {:.6f}, P1: {:.6f}, P5: {:.6f}'.format(MAP, MRR, P1, P5))\n",
    "        \n",
    "        print('This epoch took: {:.6f}'.format(time.time() - lasttime))\n",
    "        lasttime = time.time()\n",
    "\n",
    "        \n",
    "def run_epoch(data, is_training, model, optimizer):\n",
    "    '''\n",
    "    Train model for one pass of train data, and return loss, acccuracy\n",
    "    '''\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        data,\n",
    "        batch_size=40,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        drop_last=False)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        pid_title = torch.unsqueeze(Variable(batch['pid_title']), 1)\n",
    "        pid_body = torch.unsqueeze(Variable(batch['pid_body']), 1)\n",
    "        rest_title = Variable(batch['rest_title'])\n",
    "        rest_body = Variable(batch['rest_body'])\n",
    "        \n",
    "        pid_title_pad = torch.unsqueeze(Variable(batch['pid_title_pad']), 1)\n",
    "        pid_body_pad = torch.unsqueeze(Variable(batch['pid_body_pad']), 1)\n",
    "        rest_title_pad = Variable(batch['rest_title_pad'])\n",
    "        rest_body_pad = Variable(batch['rest_body_pad'])\n",
    "        \n",
    "        pid_title, pid_body = pid_title.cuda(), pid_body.cuda()\n",
    "        rest_title, rest_body = rest_title.cuda(), rest_body.cuda()\n",
    "        pid_title_pad, pid_body_pad = pid_title_pad.cuda(), pid_body_pad.cuda()\n",
    "        rest_title_pad, rest_body_pad = rest_title_pad.cuda(), rest_body_pad.cuda()\n",
    "        \n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        pt = model(pid_title)\n",
    "        pb = model(pid_body)\n",
    "        rt = model(rest_title)\n",
    "        rb = model(rest_body)\n",
    "        \n",
    "        pt = normalize(pt, 2)\n",
    "        pb = normalize(pb, 2)\n",
    "        rt = normalize(rt, 2)\n",
    "        rb = normalize(rb, 2)\n",
    "        \n",
    "        # we need to take the mean pooling taking into account the padding\n",
    "        # tensors are of dim batch_size x samples x output_size x (len - kernel + 1)\n",
    "        # pad tensors are of dim batch_size x samples x (len - kernel + 1)\n",
    "        \n",
    "        pid_title_pad_ex = torch.unsqueeze(pid_title_pad, 2).expand_as(pt)\n",
    "        pid_body_pad_ex = torch.unsqueeze(pid_body_pad, 2).expand_as(pb)\n",
    "        rest_title_pad_ex = torch.unsqueeze(rest_title_pad, 2).expand_as(rt)\n",
    "        rest_body_pad_ex = torch.unsqueeze(rest_body_pad, 2).expand_as(rb)\n",
    "        \n",
    "        pt = torch.sum(pt * pid_title_pad_ex, dim = 3)\n",
    "        pb = torch.sum(pb * pid_body_pad_ex, dim = 3)\n",
    "        rt = torch.sum(rt * rest_title_pad_ex, dim = 3)\n",
    "        rb = torch.sum(rb * rest_body_pad_ex, dim = 3)\n",
    "\n",
    "        # tensors are not of dim batch_size x samples x output_size\n",
    "        # need to scale down because not all uniformly padded\n",
    "        \n",
    "        ptp_norm = torch.unsqueeze(torch.sum(pid_title_pad, dim = 2).clamp(min = 1), 2).expand_as(pt)\n",
    "        pbp_norm = torch.unsqueeze(torch.sum(pid_body_pad, dim = 2).clamp(min = 1), 2).expand_as(pb)\n",
    "        rtp_norm = torch.unsqueeze(torch.sum(rest_title_pad, dim = 2).clamp(min = 1), 2).expand_as(rt)\n",
    "        rbp_norm = torch.unsqueeze(torch.sum(rest_body_pad, dim = 2).clamp(min = 1), 2).expand_as(rb)\n",
    "        \n",
    "        pt = pt / ptp_norm\n",
    "        pb = pb / pbp_norm\n",
    "        rt = rt / rtp_norm\n",
    "        rb = rb / rbp_norm\n",
    "        \n",
    "        pid_tensor = (pt + pb)/2\n",
    "        rest_tensor = (rt + rb)/2\n",
    "        \n",
    "        if is_training:\n",
    "            dropout = nn.Dropout(p = 0.2)\n",
    "            # we don't need to re-scale these on eval because its just cos sim\n",
    "            pid_tensor = dropout(pid_tensor)\n",
    "            rest_tensor = dropout(rest_tensor)\n",
    "        \n",
    "        if is_training:\n",
    "            loss = loss_function(pid_tensor, rest_tensor, margin = 1.0)\n",
    "            loss.backward()\n",
    "            losses.append(loss.cpu().data[0])\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            expanded = pid_tensor.expand_as(rest_tensor)\n",
    "            similarity = cs(expanded, rest_tensor, dim=2)\n",
    "            similarity = similarity.data.cpu().numpy()\n",
    "            #return similarity\n",
    "            labels = batch['labels'].numpy()\n",
    "            l = dr.convert(similarity, labels)\n",
    "            losses.extend(l)\n",
    "\n",
    "    # Calculate epoch level scores\n",
    "    if is_training:\n",
    "        avg_loss = np.mean(losses)\n",
    "        return avg_loss\n",
    "    else:\n",
    "        e = Evaluation(losses)\n",
    "        MAP = e.MAP()*100\n",
    "        MRR = e.MRR()*100\n",
    "        P1 = e.Precision(1)*100\n",
    "        P5 = e.Precision(5)*100\n",
    "        return (MAP, MRR, P1, P5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(310, embedding_tensor, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = \"../data/test.txt\"\n",
    "val = dr.read_annotations(val_path, K_neg = -1, prune_pos_cnt = -1)\n",
    "val_ex = dr.create_dev_set(ids_corpus, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Epoch 1:\n",
      "\n",
      "Train loss: 0.954059\n",
      "Val MAP: 56.448763, MRR: 68.384175, P1: 53.225806, P5: 42.688172\n",
      "This epoch took: 270.471520\n",
      "-------------\n",
      "Epoch 2:\n",
      "\n",
      "Train loss: 0.851741\n",
      "Val MAP: 56.503193, MRR: 68.544435, P1: 54.301075, P5: 41.827957\n",
      "This epoch took: 300.624090\n",
      "-------------\n",
      "Epoch 3:\n",
      "\n",
      "Train loss: 0.807282\n",
      "Val MAP: 57.388543, MRR: 69.458860, P1: 53.225806, P5: 43.870968\n",
      "This epoch took: 351.887605\n",
      "-------------\n",
      "Epoch 4:\n",
      "\n",
      "Train loss: 0.774704\n",
      "Val MAP: 57.607602, MRR: 70.851409, P1: 56.451613, P5: 42.903226\n",
      "This epoch took: 353.230862\n",
      "-------------\n",
      "Epoch 5:\n",
      "\n",
      "Train loss: 0.747547\n",
      "Val MAP: 57.016118, MRR: 69.266900, P1: 53.763441, P5: 43.548387\n",
      "This epoch took: 375.707387\n",
      "-------------\n",
      "Epoch 6:\n",
      "\n",
      "Train loss: 0.723780\n",
      "Val MAP: 57.981898, MRR: 69.133700, P1: 54.838710, P5: 44.193548\n",
      "This epoch took: 378.340500\n",
      "-------------\n",
      "Epoch 7:\n",
      "\n",
      "Train loss: 0.701311\n",
      "Val MAP: 56.591714, MRR: 68.191631, P1: 53.763441, P5: 43.333333\n",
      "This epoch took: 358.313595\n",
      "-------------\n",
      "Epoch 8:\n",
      "\n",
      "Train loss: 0.682587\n",
      "Val MAP: 56.422493, MRR: 67.604402, P1: 51.612903, P5: 43.440860\n",
      "This epoch took: 379.241347\n",
      "-------------\n",
      "Epoch 9:\n",
      "\n",
      "Train loss: 0.666059\n",
      "Val MAP: 56.691527, MRR: 69.102777, P1: 54.838710, P5: 42.795699\n",
      "This epoch took: 378.576109\n",
      "-------------\n",
      "Epoch 10:\n",
      "\n",
      "Train loss: 0.649727\n",
      "Val MAP: 55.819850, MRR: 68.631591, P1: 53.225806, P5: 42.043011\n",
      "This epoch took: 314.322543\n",
      "-------------\n",
      "Epoch 11:\n",
      "\n",
      "Train loss: 0.633687\n",
      "Val MAP: 56.268231, MRR: 69.422604, P1: 55.376344, P5: 41.935484\n",
      "This epoch took: 327.154459\n",
      "-------------\n",
      "Epoch 12:\n",
      "\n",
      "Train loss: 0.621230\n",
      "Val MAP: 56.271488, MRR: 69.956851, P1: 56.451613, P5: 41.827957\n",
      "This epoch took: 272.797434\n",
      "-------------\n",
      "Epoch 13:\n",
      "\n",
      "Train loss: 0.609553\n",
      "Val MAP: 55.684185, MRR: 68.450148, P1: 54.838710, P5: 41.935484\n",
      "This epoch took: 326.605594\n",
      "-------------\n",
      "Epoch 14:\n",
      "\n",
      "Train loss: 0.598004\n",
      "Val MAP: 55.768880, MRR: 67.858684, P1: 53.763441, P5: 41.612903\n",
      "This epoch took: 271.868613\n",
      "-------------\n",
      "Epoch 15:\n",
      "\n",
      "Train loss: 0.587240\n",
      "Val MAP: 55.726576, MRR: 68.283207, P1: 53.763441, P5: 42.365591\n",
      "This epoch took: 326.408519\n",
      "-------------\n",
      "Epoch 16:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = train_model(train_ex, val_ex, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current best model is model_epoch6\n",
    "# MAP: 57.981898, MRR: 69.133700, P1: 54.838710, P5: 44.193548\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58.05577513494322, 70.60486397787984, 58.201058201058196, 45.60846560846563)\n",
      "(57.98189767697066, 69.13369980645021, 54.83870967741935, 44.19354838709681)\n"
     ]
    }
   ],
   "source": [
    "# \"test\" on dev set\n",
    "model = torch.load(\"model_epoch6\")\n",
    "\n",
    "val_path = \"../data/dev.txt\"\n",
    "val = dr.read_annotations(val_path, K_neg = -1, prune_pos_cnt = -1)\n",
    "val_ex = dr.create_dev_set(ids_corpus, val)\n",
    "\n",
    "model = model.cuda()\n",
    "z = run_epoch(val_ex, False, model, None)\n",
    "print(z)\n",
    "\n",
    "# test set\n",
    "val_path = \"../data/test.txt\"\n",
    "val = dr.read_annotations(val_path, K_neg = -1, prune_pos_cnt = -1)\n",
    "val_ex = dr.create_dev_set(ids_corpus, val)\n",
    "\n",
    "z = run_epoch(val_ex, False, model, None)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
